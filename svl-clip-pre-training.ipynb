{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5098493,"sourceType":"datasetVersion","datasetId":2960936}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install ftfy regex tqdm -q\n! pip install git+https://github.com/openai/CLIP.git -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T08:29:14.700435Z","iopub.execute_input":"2024-05-18T08:29:14.700737Z","iopub.status.idle":"2024-05-18T08:29:44.481509Z","shell.execute_reply.started":"2024-05-18T08:29:14.700711Z","shell.execute_reply":"2024-05-18T08:29:44.480362Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import clip\nimport torch\nimport torchvision\nimport numpy as np\nimport tqdm\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:29:44.483386Z","iopub.execute_input":"2024-05-18T08:29:44.483694Z","iopub.status.idle":"2024-05-18T08:29:51.138351Z","shell.execute_reply.started":"2024-05-18T08:29:44.483663Z","shell.execute_reply":"2024-05-18T08:29:51.137531Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"clip.available_models()\n\n# Load the model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load('RN50', device)\ninput_resolution = model.visual.input_resolution\ncontext_length = model.context_length\nvocab_size = model.vocab_size\n\nprint(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\nprint(\"Input resolution:\", input_resolution)\nprint(\"Context length:\", context_length)\nprint(\"Vocab size:\", vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:29:51.139395Z","iopub.execute_input":"2024-05-18T08:29:51.139757Z","iopub.status.idle":"2024-05-18T08:30:00.853054Z","shell.execute_reply.started":"2024-05-18T08:29:51.139733Z","shell.execute_reply":"2024-05-18T08:30:00.852084Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"100%|███████████████████████████████████████| 244M/244M [00:05<00:00, 46.7MiB/s]\n","output_type":"stream"},{"name":"stdout","text":"Model parameters: 102,007,137\nInput resolution: 224\nContext length: 77\nVocab size: 49408\n","output_type":"stream"}]},{"cell_type":"code","source":"train_path_to_data = '/kaggle/input/siri-whu-data-set/'\ntrain_dataset = torchvision.datasets.ImageFolder(train_path_to_data, transform=preprocess)\n# 展示读入数据集的类别等信息\nclassnames = train_dataset.classes\nprint(\"Class names: {}\".format(classnames))\nprint(\"Total number of classes: {}\".format(len(classnames)))\nprint(train_dataset.class_to_idx) # 类别离散数字化，用0-11表示对应类\n# loader = torch.utils.data.DataLoader(images, batch_size=32, num_workers=2)\n\ntemplates = [\n    'a centered satellite photo of {}.',\n    'a centered satellite photo of a {}.',\n    'a centered satellite photo of the {}.',\n]\n\nprint(f\"{len(classnames)} classes, {len(templates)} templates\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:30:00.854931Z","iopub.execute_input":"2024-05-18T08:30:00.855251Z","iopub.status.idle":"2024-05-18T08:30:01.416728Z","shell.execute_reply.started":"2024-05-18T08:30:00.855224Z","shell.execute_reply":"2024-05-18T08:30:01.415727Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Class names: ['agriculture', 'commercial', 'harbor', 'idle_land', 'industrial', 'meadow', 'overpass', 'park', 'pond', 'residential', 'river', 'water']\nTotal number of classes: 12\n{'agriculture': 0, 'commercial': 1, 'harbor': 2, 'idle_land': 3, 'industrial': 4, 'meadow': 5, 'overpass': 6, 'park': 7, 'pond': 8, 'residential': 9, 'river': 10, 'water': 11}\n12 classes, 3 templates\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 64\n_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ndef get_features(model, _data_loader, model_type = 'clip'):\n    all_features = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for i, (images, labels) in enumerate(_data_loader):\n            if model_type == 'clip': features = model.encode_image(images.to(device))\n            else: features = model.backbone(images.to(device)).flatten(start_dim=1)\n            all_features.append(features)\n            all_labels.append(labels)\n\n    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n\ntrain_features, train_labels = get_features(model, _data_loader)\nprint(len(train_features[0]))","metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:30:01.418027Z","iopub.execute_input":"2024-05-18T08:30:01.418413Z","iopub.status.idle":"2024-05-18T08:30:18.903669Z","shell.execute_reply.started":"2024-05-18T08:30:01.418378Z","shell.execute_reply":"2024-05-18T08:30:18.902786Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"1024\n","output_type":"stream"}]},{"cell_type":"code","source":"feature_to_path = '/kaggle/working/pretrained_features'\npseudo = ''\nos.makedirs(feature_to_path, exist_ok=True)\nfeature_save_filename = \"{}_features{}\".format('siri_whu_ds',pseudo)\n\nnp.savez(\n    os.path.join(feature_to_path, feature_save_filename),\n    train_features,\n    label_list=train_labels,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T08:34:32.453140Z","iopub.execute_input":"2024-05-18T08:34:32.453808Z","iopub.status.idle":"2024-05-18T08:34:32.468342Z","shell.execute_reply.started":"2024-05-18T08:34:32.453777Z","shell.execute_reply":"2024-05-18T08:34:32.467555Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!pip install lightly -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note: The model and training settings do not follow the reference settings\n# from the paper. The settings are chosen such that the example can easily be\n# run on a small dataset with a single GPU.\n\nimport torch\nimport torchvision\nfrom torch import nn\n\nfrom lightly.loss import NTXentLoss\nfrom lightly.models.modules import SimCLRProjectionHead\nfrom lightly.transforms.simclr_transform import SimCLRTransform\n\n\nclass SimCLR(nn.Module):\n    def __init__(self, backbone):\n        super().__init__()\n        self.backbone = backbone\n        self.projection_head = SimCLRProjectionHead(512, 512, 128)\n\n    def forward(self, x):\n        x = self.backbone(x).flatten(start_dim=1)\n        z = self.projection_head(x)\n        return z\n\n\nresnet = torchvision.models.resnet18()\nbackbone = nn.Sequential(*list(resnet.children())[:-1])\nssl_model = SimCLR(backbone)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nssl_model.to(device)\n\nssl_transform = SimCLRTransform(input_size=128, gaussian_blur=0.0)\nssl_dataset = torchvision.datasets.ImageFolder(train_path_to_data, transform=ssl_transform)\nssl_dataloader = torch.utils.data.DataLoader(\n    ssl_dataset,\n    batch_size=128,\n    shuffle=True,\n    drop_last=True,\n    num_workers=4,\n)\n\ncriterion = NTXentLoss()\noptimizer = torch.optim.SGD(ssl_model.parameters(), lr=0.06)\n\nprint(\"Starting Training\")\nfor epoch in range(10):\n    total_loss = 0\n    for batch in ssl_dataloader:\n        x0, x1 = batch[0]\n        x0 = x0.to(device)\n        x1 = x1.to(device)\n        z0 = ssl_model(x0)\n        z1 = ssl_model(x1)\n        loss = criterion(z0, z1)\n        total_loss += loss.detach()\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    avg_loss = total_loss / len(ssl_dataloader)\n    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:02:09.416082Z","iopub.execute_input":"2024-05-18T09:02:09.416469Z","iopub.status.idle":"2024-05-18T09:03:27.902208Z","shell.execute_reply.started":"2024-05-18T09:02:09.416432Z","shell.execute_reply":"2024-05-18T09:03:27.901016Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Starting Training\nepoch: 00, loss: 5.43341\nepoch: 01, loss: 5.36741\nepoch: 02, loss: 5.30327\nepoch: 03, loss: 5.22993\nepoch: 04, loss: 5.23047\nepoch: 05, loss: 5.19741\nepoch: 06, loss: 5.17265\nepoch: 07, loss: 5.14395\nepoch: 08, loss: 5.12276\nepoch: 09, loss: 5.07642\n","output_type":"stream"}]},{"cell_type":"code","source":"train_features, train_labels = get_features(ssl_model, _data_loader, model_type = 'SSL')\nprint(len(train_features[0]))","metadata":{"execution":{"iopub.status.busy":"2024-05-18T09:03:27.904302Z","iopub.execute_input":"2024-05-18T09:03:27.904628Z","iopub.status.idle":"2024-05-18T09:03:27.949688Z","shell.execute_reply.started":"2024-05-18T09:03:27.904595Z","shell.execute_reply":"2024-05-18T09:03:27.948411Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_features, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mssl_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSSL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_features[\u001b[38;5;241m0\u001b[39m]))\n","\u001b[0;31mTypeError\u001b[0m: get_features() got an unexpected keyword argument 'model_type'"],"ename":"TypeError","evalue":"get_features() got an unexpected keyword argument 'model_type'","output_type":"error"}]}]}